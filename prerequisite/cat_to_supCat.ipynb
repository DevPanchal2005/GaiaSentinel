{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class ID Remapping Automation Script\n",
    "\n",
    "This notebook automates the process of remapping class IDs in YOLO annotation files.\n",
    "It processes all annotation files in the `data/labels/` directory and creates updated files in `data/new_labels/`.\n",
    "\n",
    "## Features:\n",
    "- Processes all .txt annotation files in the labels directory\n",
    "- Remaps class IDs according to the provided mapping dictionary\n",
    "- Creates a backup of original files\n",
    "- Provides detailed logging and progress tracking\n",
    "- Handles errors gracefully\n",
    "- Generates a summary report of changes made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ID Remapping Script Initialized\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Class ID Remapping Script Initialized\")\n",
    "print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded class mapping with 60 entries\n",
      "Original classes: 0 to 59\n",
      "New classes: 0 to 27\n"
     ]
    }
   ],
   "source": [
    "# Define the class mapping dictionary\n",
    "CLASS_MAPPING = {\n",
    "    0: (0, 'Aluminium foil'),\n",
    "    1: (1, 'Battery'),\n",
    "    2: (2, 'Blister pack'),\n",
    "    3: (2, 'Blister pack'),\n",
    "    4: (3, 'Bottle'),\n",
    "    5: (3, 'Bottle'),\n",
    "    6: (3, 'Bottle'),\n",
    "    7: (4, 'Bottle cap'),\n",
    "    8: (4, 'Bottle cap'),\n",
    "    9: (5, 'Broken glass'),\n",
    "    10: (6, 'Can'),\n",
    "    11: (6, 'Can'),\n",
    "    12: (6, 'Can'),\n",
    "    13: (7, 'Carton'),\n",
    "    14: (7, 'Carton'),\n",
    "    15: (7, 'Carton'),\n",
    "    16: (7, 'Carton'),\n",
    "    17: (7, 'Carton'),\n",
    "    18: (7, 'Carton'),\n",
    "    19: (7, 'Carton'),\n",
    "    20: (8, 'Cup'),\n",
    "    21: (8, 'Cup'),\n",
    "    22: (8, 'Cup'),\n",
    "    23: (8, 'Cup'),\n",
    "    24: (8, 'Cup'),\n",
    "    25: (9, 'Food waste'),\n",
    "    26: (10, 'Glass jar'),\n",
    "    27: (11, 'Lid'),\n",
    "    28: (11, 'Lid'),\n",
    "    29: (12, 'Other plastic'),\n",
    "    30: (13, 'Paper'),\n",
    "    31: (13, 'Paper'),\n",
    "    32: (13, 'Paper'),\n",
    "    33: (13, 'Paper'),\n",
    "    34: (14, 'Paper bag'),\n",
    "    35: (14, 'Paper bag'),\n",
    "    36: (15, 'Plastic bag & wrapper'),\n",
    "    37: (15, 'Plastic bag & wrapper'),\n",
    "    38: (15, 'Plastic bag & wrapper'),\n",
    "    39: (15, 'Plastic bag & wrapper'),\n",
    "    40: (15, 'Plastic bag & wrapper'),\n",
    "    41: (15, 'Plastic bag & wrapper'),\n",
    "    42: (15, 'Plastic bag & wrapper'),\n",
    "    43: (16, 'Plastic container'),\n",
    "    44: (16, 'Plastic container'),\n",
    "    45: (16, 'Plastic container'),\n",
    "    46: (16, 'Plastic container'),\n",
    "    47: (16, 'Plastic container'),\n",
    "    48: (17, 'Plastic glooves'),\n",
    "    49: (18, 'Plastic utensils'),\n",
    "    50: (19, 'Pop tab'),\n",
    "    51: (20, 'Rope & strings'),\n",
    "    52: (21, 'Scrap metal'),\n",
    "    53: (22, 'Shoe'),\n",
    "    54: (23, 'Squeezable tube'),\n",
    "    55: (24, 'Straw'),\n",
    "    56: (24, 'Straw'),\n",
    "    57: (25, 'Styrofoam piece'),\n",
    "    58: (26, 'Unlabeled litter'),\n",
    "    59: (27, 'Cigarette')\n",
    "}\n",
    "\n",
    "print(f\"Loaded class mapping with {len(CLASS_MAPPING)} entries\")\n",
    "print(f\"Original classes: {min(CLASS_MAPPING.keys())} to {max(CLASS_MAPPING.keys())}\")\n",
    "print(f\"New classes: {min([v[0] for v in CLASS_MAPPING.values()])} to {max([v[0] for v in CLASS_MAPPING.values()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: data\\new_labels\n",
      "Input directory: data\\labels\n",
      "Directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "LABELS_DIR = Path('data/labels')\n",
    "NEW_LABELS_DIR = Path('data/new_labels')\n",
    "BACKUP_DIR = Path('data/labels_backup')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "NEW_LABELS_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Output directory: {NEW_LABELS_DIR}\")\n",
    "\n",
    "# Verify input directory exists\n",
    "if not LABELS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Labels directory not found: {LABELS_DIR}\")\n",
    "\n",
    "print(f\"Input directory: {LABELS_DIR}\")\n",
    "print(f\"Directory exists: {LABELS_DIR.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotation_file(input_path, output_path, class_mapping):\n",
    "    \"\"\"\n",
    "    Process a single YOLO annotation file and remap class IDs.\n",
    "    \n",
    "    Args:\n",
    "        input_path (Path): Path to input annotation file\n",
    "        output_path (Path): Path to output annotation file\n",
    "        class_mapping (dict): Dictionary mapping old class IDs to (new_id, class_name)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics about the processing (lines processed, changes made, etc.)\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'lines_processed': 0,\n",
    "        'lines_changed': 0,\n",
    "        'original_classes': set(),\n",
    "        'new_classes': set(),\n",
    "        'errors': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(input_path, 'r') as infile, open(output_path, 'w') as outfile:\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Skip empty lines\n",
    "                if not line:\n",
    "                    outfile.write('\\n')\n",
    "                    continue\n",
    "                \n",
    "                stats['lines_processed'] += 1\n",
    "                \n",
    "                try:\n",
    "                    # Parse YOLO format: class_id x_center y_center width height\n",
    "                    parts = line.split()\n",
    "                    if len(parts) != 5:\n",
    "                        stats['errors'].append(f\"Line {line_num}: Invalid format - expected 5 values, got {len(parts)}\")\n",
    "                        outfile.write(line + '\\n')\n",
    "                        continue\n",
    "                    \n",
    "                    original_class_id = int(parts[0])\n",
    "                    stats['original_classes'].add(original_class_id)\n",
    "                    \n",
    "                    # Check if class ID exists in mapping\n",
    "                    if original_class_id not in class_mapping:\n",
    "                        stats['errors'].append(f\"Line {line_num}: Class ID {original_class_id} not found in mapping\")\n",
    "                        outfile.write(line + '\\n')\n",
    "                        continue\n",
    "                    \n",
    "                    # Get new class ID\n",
    "                    new_class_id, class_name = class_mapping[original_class_id]\n",
    "                    stats['new_classes'].add(new_class_id)\n",
    "                    \n",
    "                    # Update the line with new class ID\n",
    "                    parts[0] = str(new_class_id)\n",
    "                    new_line = ' '.join(parts)\n",
    "                    outfile.write(new_line + '\\n')\n",
    "                    \n",
    "                    if original_class_id != new_class_id:\n",
    "                        stats['lines_changed'] += 1\n",
    "                        \n",
    "                except ValueError as e:\n",
    "                    stats['errors'].append(f\"Line {line_num}: Error parsing class ID - {str(e)}\")\n",
    "                    outfile.write(line + '\\n')\n",
    "                    continue\n",
    "                    \n",
    "    except Exception as e:\n",
    "        stats['errors'].append(f\"File processing error: {str(e)}\")\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 annotation files to process\n",
      "First few files: ['batch_10_000000.txt', 'batch_10_000001.txt', 'batch_10_000002.txt', 'batch_10_000003.txt', 'batch_10_000004.txt']\n",
      "... and 1495 more files\n"
     ]
    }
   ],
   "source": [
    "# Get list of all annotation files\n",
    "annotation_files = list(LABELS_DIR.glob('*.txt'))\n",
    "print(f\"Found {len(annotation_files)} annotation files to process\")\n",
    "\n",
    "if len(annotation_files) == 0:\n",
    "    print(\"No .txt files found in the labels directory!\")\n",
    "else:\n",
    "    print(f\"First few files: {[f.name for f in annotation_files[:5]]}\")\n",
    "    if len(annotation_files) > 5:\n",
    "        print(f\"... and {len(annotation_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting file processing...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1500/1500 [00:16<00:00, 91.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File processing completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracking variables\n",
    "total_stats = {\n",
    "    'files_processed': 0,\n",
    "    'files_with_errors': 0,\n",
    "    'total_lines_processed': 0,\n",
    "    'total_lines_changed': 0,\n",
    "    'all_original_classes': set(),\n",
    "    'all_new_classes': set(),\n",
    "    'all_errors': []\n",
    "}\n",
    "\n",
    "file_stats = []\n",
    "\n",
    "print(\"Starting file processing...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Process each annotation file\n",
    "for input_file in tqdm(annotation_files, desc=\"Processing files\"):\n",
    "    output_file = NEW_LABELS_DIR / input_file.name\n",
    "    \n",
    "    try:\n",
    "        # Process the file\n",
    "        stats = process_annotation_file(input_file, output_file, CLASS_MAPPING)\n",
    "        \n",
    "        # Update total statistics\n",
    "        total_stats['files_processed'] += 1\n",
    "        total_stats['total_lines_processed'] += stats['lines_processed']\n",
    "        total_stats['total_lines_changed'] += stats['lines_changed']\n",
    "        total_stats['all_original_classes'].update(stats['original_classes'])\n",
    "        total_stats['all_new_classes'].update(stats['new_classes'])\n",
    "        \n",
    "        if stats['errors']:\n",
    "            total_stats['files_with_errors'] += 1\n",
    "            total_stats['all_errors'].extend([f\"{input_file.name}: {error}\" for error in stats['errors']])\n",
    "        \n",
    "        # Store individual file stats\n",
    "        file_stats.append({\n",
    "            'filename': input_file.name,\n",
    "            'lines_processed': stats['lines_processed'],\n",
    "            'lines_changed': stats['lines_changed'],\n",
    "            'original_classes': len(stats['original_classes']),\n",
    "            'new_classes': len(stats['new_classes']),\n",
    "            'errors': len(stats['errors'])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {input_file.name}: {str(e)}\")\n",
    "        total_stats['files_with_errors'] += 1\n",
    "        total_stats['all_errors'].append(f\"{input_file.name}: {str(e)}\")\n",
    "\n",
    "print(\"\\nFile processing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING SUMMARY\n",
      "============================================================\n",
      "Files processed: 1500\n",
      "Files with errors: 0\n",
      "Total annotation lines processed: 4784\n",
      "Total annotation lines changed: 4714\n",
      "Change percentage: 98.54%\n",
      "\n",
      "Original classes found: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "New classes created: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "\n",
      "✅ No errors encountered!\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROCESSING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Files processed: {total_stats['files_processed']}\")\n",
    "print(f\"Files with errors: {total_stats['files_with_errors']}\")\n",
    "print(f\"Total annotation lines processed: {total_stats['total_lines_processed']}\")\n",
    "print(f\"Total annotation lines changed: {total_stats['total_lines_changed']}\")\n",
    "print(f\"Change percentage: {(total_stats['total_lines_changed'] / max(total_stats['total_lines_processed'], 1)) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOriginal classes found: {sorted(total_stats['all_original_classes'])}\")\n",
    "print(f\"New classes created: {sorted(total_stats['all_new_classes'])}\")\n",
    "\n",
    "if total_stats['all_errors']:\n",
    "    print(f\"\\nErrors encountered ({len(total_stats['all_errors'])}):\") \n",
    "    for error in total_stats['all_errors'][:10]:  # Show first 10 errors\n",
    "        print(f\"  - {error}\")\n",
    "    if len(total_stats['all_errors']) > 10:\n",
    "        print(f\"  ... and {len(total_stats['all_errors']) - 10} more errors\")\n",
    "else:\n",
    "    print(\"\\n✅ No errors encountered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DETAILED FILE STATISTICS\n",
      "============================================================\n",
      "\n",
      "Summary statistics:\n",
      "       lines_processed  lines_changed  original_classes  new_classes  errors\n",
      "count      1500.000000    1500.000000       1500.000000  1500.000000  1500.0\n",
      "mean          3.189333       3.142667          2.082000     1.984667     0.0\n",
      "std           4.691712       4.684258          1.491563     1.320705     0.0\n",
      "min           1.000000       0.000000          1.000000     1.000000     0.0\n",
      "25%           1.000000       1.000000          1.000000     1.000000     0.0\n",
      "50%           2.000000       2.000000          2.000000     2.000000     0.0\n",
      "75%           3.000000       3.000000          2.000000     2.000000     0.0\n",
      "max          90.000000      90.000000         13.000000    11.000000     0.0\n",
      "\n",
      "Files with most changes:\n",
      "           filename  lines_processed  lines_changed\n",
      " batch_6_000072.txt               90             90\n",
      " batch_8_000021.txt               54             54\n",
      "batch_12_000061.txt               38             38\n",
      "batch_12_000062.txt               37             37\n",
      "batch_13_000025.txt               37             37\n",
      " batch_2_000048.txt               37             37\n",
      " batch_6_000066.txt               30             30\n",
      "batch_15_000029.txt               29             29\n",
      " batch_8_000065.txt               28             28\n",
      "batch_12_000025.txt               27             27\n"
     ]
    }
   ],
   "source": [
    "# Create detailed statistics DataFrame\n",
    "if file_stats:\n",
    "    df_stats = pd.DataFrame(file_stats)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED FILE STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(df_stats.describe())\n",
    "    \n",
    "    print(\"\\nFiles with most changes:\")\n",
    "    top_changed = df_stats.nlargest(10, 'lines_changed')[['filename', 'lines_processed', 'lines_changed']]\n",
    "    print(top_changed.to_string(index=False))\n",
    "    \n",
    "    if df_stats['errors'].sum() > 0:\n",
    "        print(\"\\nFiles with errors:\")\n",
    "        error_files = df_stats[df_stats['errors'] > 0][['filename', 'errors']]\n",
    "        print(error_files.to_string(index=False))\n",
    "else:\n",
    "    print(\"No file statistics available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASS MAPPING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Class consolidation summary:\n",
      "Class  0 (Aluminium foil           ):  1 original classes [0]\n",
      "Class  1 (Battery                  ):  1 original classes [1]\n",
      "Class  2 (Blister pack             ):  2 original classes [2, 3]\n",
      "Class  3 (Bottle                   ):  3 original classes [4, 5, 6]\n",
      "Class  4 (Bottle cap               ):  2 original classes [7, 8]\n",
      "Class  5 (Broken glass             ):  1 original classes [9]\n",
      "Class  6 (Can                      ):  3 original classes [10, 11, 12]\n",
      "Class  7 (Carton                   ):  7 original classes [13, 14, 15, 16, 17, 18, 19]\n",
      "Class  8 (Cup                      ):  5 original classes [20, 21, 22, 23, 24]\n",
      "Class  9 (Food waste               ):  1 original classes [25]\n",
      "Class 10 (Glass jar                ):  1 original classes [26]\n",
      "Class 11 (Lid                      ):  2 original classes [27, 28]\n",
      "Class 12 (Other plastic            ):  1 original classes [29]\n",
      "Class 13 (Paper                    ):  4 original classes [30, 31, 32, 33]\n",
      "Class 14 (Paper bag                ):  2 original classes [34, 35]\n",
      "Class 15 (Plastic bag & wrapper    ):  7 original classes [36, 37, 38, 39, 40, 41, 42]\n",
      "Class 16 (Plastic container        ):  5 original classes [43, 44, 45, 46, 47]\n",
      "Class 17 (Plastic glooves          ):  1 original classes [48]\n",
      "Class 18 (Plastic utensils         ):  1 original classes [49]\n",
      "Class 19 (Pop tab                  ):  1 original classes [50]\n",
      "Class 20 (Rope & strings           ):  1 original classes [51]\n",
      "Class 21 (Scrap metal              ):  1 original classes [52]\n",
      "Class 22 (Shoe                     ):  1 original classes [53]\n",
      "Class 23 (Squeezable tube          ):  1 original classes [54]\n",
      "Class 24 (Straw                    ):  2 original classes [55, 56]\n",
      "Class 25 (Styrofoam piece          ):  1 original classes [57]\n",
      "Class 26 (Unlabeled litter         ):  1 original classes [58]\n",
      "Class 27 (Cigarette                ):  1 original classes [59]\n",
      "\n",
      "Total reduction: 60 → 28 classes\n",
      "Reduction percentage: 53.3%\n"
     ]
    }
   ],
   "source": [
    "# Create class mapping summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASS MAPPING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count how many original classes map to each new class\n",
    "new_class_counts = defaultdict(list)\n",
    "for old_id, (new_id, class_name) in CLASS_MAPPING.items():\n",
    "    new_class_counts[new_id].append((old_id, class_name))\n",
    "\n",
    "print(\"\\nClass consolidation summary:\")\n",
    "for new_id in sorted(new_class_counts.keys()):\n",
    "    old_ids, class_name = zip(*new_class_counts[new_id])\n",
    "    class_name = class_name[0]  # All should be the same\n",
    "    print(f\"Class {new_id:2d} ({class_name:25s}): {len(old_ids):2d} original classes {sorted(old_ids)}\")\n",
    "\n",
    "print(f\"\\nTotal reduction: {len(CLASS_MAPPING)} → {len(new_class_counts)} classes\")\n",
    "print(f\"Reduction percentage: {((len(CLASS_MAPPING) - len(new_class_counts)) / len(CLASS_MAPPING)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VERIFICATION SAMPLES\n",
      "============================================================\n",
      "\n",
      "📁 File: batch_10_000000.txt\n",
      "----------------------------------------\n",
      "Original annotations: 1\n",
      "Processed annotations: 1\n",
      "  Line 1: 5 → 3\n",
      "\n",
      "📁 File: batch_10_000001.txt\n",
      "----------------------------------------\n",
      "Original annotations: 13\n",
      "Processed annotations: 13\n",
      "  Line 1: 4 → 3\n",
      "  Line 2: 7 → 4\n",
      "  Line 3: 58 → 26\n",
      "  ... and 10 more lines\n",
      "\n",
      "📁 File: batch_10_000002.txt\n",
      "----------------------------------------\n",
      "Original annotations: 6\n",
      "Processed annotations: 6\n",
      "  Line 1: 58 → 26\n",
      "  Line 2: 29 → 12\n",
      "  Line 3: 5 → 3\n",
      "  ... and 3 more lines\n"
     ]
    }
   ],
   "source": [
    "# Verify a few processed files\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION SAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show before/after comparison for first few files\n",
    "sample_files = annotation_files[:3]\n",
    "\n",
    "for sample_file in sample_files:\n",
    "    print(f\"\\n📁 File: {sample_file.name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Read original file\n",
    "    try:\n",
    "        with open(sample_file, 'r') as f:\n",
    "            original_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        \n",
    "        # Read processed file\n",
    "        processed_file = NEW_LABELS_DIR / sample_file.name\n",
    "        with open(processed_file, 'r') as f:\n",
    "            processed_lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        \n",
    "        print(f\"Original annotations: {len(original_lines)}\")\n",
    "        print(f\"Processed annotations: {len(processed_lines)}\")\n",
    "        \n",
    "        # Show first few lines comparison\n",
    "        for i, (orig, proc) in enumerate(zip(original_lines[:3], processed_lines[:3])):\n",
    "            orig_class = orig.split()[0]\n",
    "            proc_class = proc.split()[0]\n",
    "            change_indicator = \"→\" if orig_class != proc_class else \"=\"\n",
    "            print(f\"  Line {i+1}: {orig_class} {change_indicator} {proc_class}\")\n",
    "        \n",
    "        if len(original_lines) > 3:\n",
    "            print(f\"  ... and {len(original_lines) - 3} more lines\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading files: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Detailed report saved to: class_remapping_report.txt\n",
      "\n",
      "✅ Processing complete! Check the 'data\\new_labels' directory for updated annotation files.\n"
     ]
    }
   ],
   "source": [
    "# Save processing report\n",
    "report_path = Path('class_remapping_report.txt')\n",
    "\n",
    "with open(report_path, 'w') as report_file:\n",
    "    report_file.write(\"Class ID Remapping Report\\n\")\n",
    "    report_file.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    \n",
    "    report_file.write(f\"Processing Date: {pd.Timestamp.now()}\\n\")\n",
    "    report_file.write(f\"Input Directory: {LABELS_DIR}\\n\")\n",
    "    report_file.write(f\"Output Directory: {NEW_LABELS_DIR}\\n\\n\")\n",
    "    \n",
    "    report_file.write(\"Summary Statistics:\\n\")\n",
    "    report_file.write(f\"  Files processed: {total_stats['files_processed']}\\n\")\n",
    "    report_file.write(f\"  Files with errors: {total_stats['files_with_errors']}\\n\")\n",
    "    report_file.write(f\"  Total lines processed: {total_stats['total_lines_processed']}\\n\")\n",
    "    report_file.write(f\"  Total lines changed: {total_stats['total_lines_changed']}\\n\")\n",
    "    report_file.write(f\"  Change percentage: {(total_stats['total_lines_changed'] / max(total_stats['total_lines_processed'], 1)) * 100:.2f}%\\n\\n\")\n",
    "    \n",
    "    report_file.write(\"Class Mapping Applied:\\n\")\n",
    "    for old_id, (new_id, class_name) in sorted(CLASS_MAPPING.items()):\n",
    "        report_file.write(f\"  {old_id:2d} -> {new_id:2d} ({class_name})\\n\")\n",
    "    \n",
    "    if total_stats['all_errors']:\n",
    "        report_file.write(\"\\nErrors Encountered:\\n\")\n",
    "        for error in total_stats['all_errors']:\n",
    "            report_file.write(f\"  {error}\\n\")\n",
    "\n",
    "print(f\"\\n📄 Detailed report saved to: {report_path}\")\n",
    "print(f\"\\n✅ Processing complete! Check the '{NEW_LABELS_DIR}' directory for updated annotation files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Class names file created: data\\new_labels\\classes.txt\n",
      "\n",
      "New class list:\n",
      "   0: Aluminium foil\n",
      "   1: Battery\n",
      "   2: Blister pack\n",
      "   3: Bottle\n",
      "   4: Bottle cap\n",
      "   5: Broken glass\n",
      "   6: Can\n",
      "   7: Carton\n",
      "   8: Cup\n",
      "   9: Food waste\n",
      "  10: Glass jar\n",
      "  11: Lid\n",
      "  12: Other plastic\n",
      "  13: Paper\n",
      "  14: Paper bag\n",
      "  15: Plastic bag & wrapper\n",
      "  16: Plastic container\n",
      "  17: Plastic glooves\n",
      "  18: Plastic utensils\n",
      "  19: Pop tab\n",
      "  20: Rope & strings\n",
      "  21: Scrap metal\n",
      "  22: Shoe\n",
      "  23: Squeezable tube\n",
      "  24: Straw\n",
      "  25: Styrofoam piece\n",
      "  26: Unlabeled litter\n",
      "  27: Cigarette\n"
     ]
    }
   ],
   "source": [
    "# Optional: Create a classes.txt file with the new class names\n",
    "classes_file = NEW_LABELS_DIR / 'classes.txt'\n",
    "\n",
    "# Get unique new classes and their names\n",
    "unique_new_classes = {}\n",
    "for old_id, (new_id, class_name) in CLASS_MAPPING.items():\n",
    "    unique_new_classes[new_id] = class_name\n",
    "\n",
    "with open(classes_file, 'w') as f:\n",
    "    for class_id in sorted(unique_new_classes.keys()):\n",
    "        f.write(f\"{unique_new_classes[class_id]}\\n\")\n",
    "\n",
    "print(f\"\\n📝 Class names file created: {classes_file}\")\n",
    "print(\"\\nNew class list:\")\n",
    "for class_id in sorted(unique_new_classes.keys()):\n",
    "    print(f\"  {class_id:2d}: {unique_new_classes[class_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "1. **Run all cells sequentially** - Each cell builds on the previous ones\n",
    "2. **Check the output** - The script provides detailed progress and statistics\n",
    "3. **Review the report** - A detailed report is saved as `class_remapping_report.txt`\n",
    "4. **Verify results** - Sample comparisons are shown to verify the remapping worked correctly\n",
    "5. **Use new files** - Updated annotation files are in `data/new_labels/`\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- **`data/new_labels/*.txt`** - Updated annotation files with remapped class IDs\n",
    "- **`data/new_labels/classes.txt`** - List of new class names in order\n",
    "- **`class_remapping_report.txt`** - Detailed processing report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class_remapping_report.txt\n",
    "\n",
    "Class ID Remapping Report\n",
    "==================================================\n",
    "\n",
    "text \n",
    "Class ID Remapping Report\n",
    "==================================================\n",
    "\n",
    "Processing Date: 2025-08-06 19:06:40.942912\n",
    "Input Directory: data\\labels\n",
    "Output Directory: data\\new_labels\n",
    "\n",
    "Summary Statistics:\n",
    "  Files processed: 1500\n",
    "  Files with errors: 0\n",
    "  Total lines processed: 4784\n",
    "  Total lines changed: 4714\n",
    "  Change percentage: 98.54%\n",
    "\n",
    "Class Mapping Applied:\n",
    "   0 ->  0 (Aluminium foil)\n",
    "   1 ->  1 (Battery)\n",
    "   2 ->  2 (Blister pack)\n",
    "   3 ->  2 (Blister pack)\n",
    "   4 ->  3 (Bottle)\n",
    "   5 ->  3 (Bottle)\n",
    "   6 ->  3 (Bottle)\n",
    "   7 ->  4 (Bottle cap)\n",
    "   8 ->  4 (Bottle cap)\n",
    "   9 ->  5 (Broken glass)\n",
    "  10 ->  6 (Can)\n",
    "  11 ->  6 (Can)\n",
    "  12 ->  6 (Can)\n",
    "  13 ->  7 (Carton)\n",
    "  14 ->  7 (Carton)\n",
    "  15 ->  7 (Carton)\n",
    "  16 ->  7 (Carton)\n",
    "  17 ->  7 (Carton)\n",
    "  18 ->  7 (Carton)\n",
    "  19 ->  7 (Carton)\n",
    "  20 ->  8 (Cup)\n",
    "  21 ->  8 (Cup)\n",
    "  22 ->  8 (Cup)\n",
    "  23 ->  8 (Cup)\n",
    "  24 ->  8 (Cup)\n",
    "  25 ->  9 (Food waste)\n",
    "  26 -> 10 (Glass jar)\n",
    "  27 -> 11 (Lid)\n",
    "  28 -> 11 (Lid)\n",
    "  29 -> 12 (Other plastic)\n",
    "  30 -> 13 (Paper)\n",
    "  31 -> 13 (Paper)\n",
    "  32 -> 13 (Paper)\n",
    "  33 -> 13 (Paper)\n",
    "  34 -> 14 (Paper bag)\n",
    "  35 -> 14 (Paper bag)\n",
    "  36 -> 15 (Plastic bag & wrapper)\n",
    "  37 -> 15 (Plastic bag & wrapper)\n",
    "  38 -> 15 (Plastic bag & wrapper)\n",
    "  39 -> 15 (Plastic bag & wrapper)\n",
    "  40 -> 15 (Plastic bag & wrapper)\n",
    "  41 -> 15 (Plastic bag & wrapper)\n",
    "  42 -> 15 (Plastic bag & wrapper)\n",
    "  43 -> 16 (Plastic container)\n",
    "  44 -> 16 (Plastic container)\n",
    "  45 -> 16 (Plastic container)\n",
    "  46 -> 16 (Plastic container)\n",
    "  47 -> 16 (Plastic container)\n",
    "  48 -> 17 (Plastic glooves)\n",
    "  49 -> 18 (Plastic utensils)\n",
    "  50 -> 19 (Pop tab)\n",
    "  51 -> 20 (Rope & strings)\n",
    "  52 -> 21 (Scrap metal)\n",
    "  53 -> 22 (Shoe)\n",
    "  54 -> 23 (Squeezable tube)\n",
    "  55 -> 24 (Straw)\n",
    "  56 -> 24 (Straw)\n",
    "  57 -> 25 (Styrofoam piece)\n",
    "  58 -> 26 (Unlabeled litter)\n",
    "  59 -> 27 (Cigarette)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
